{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apache Arrow - PySpark.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOm+24MY4fEVK+tdK+wblHp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dspauloplima/PySpark/blob/main/Apache_Arrow_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4csZqt-sdIA"
      },
      "source": [
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yapNslzOsg9K"
      },
      "source": [
        "# Apach Arrow\n",
        "\n",
        "É um formato de dados colunar em memória que usa o spark para transferir dados entre a JVM e os processos Python. Interessante para quem trabalha com Pandas/Numpy.\n",
        "\n",
        "Requer algumas mudanças de confuguração ou código para ter todas as vantagens e assegurar compatibilidade.\n",
        "\n",
        "\n",
        "Pandas == 0.23.2 or higher<br>\n",
        "PyArrow == 1.0.0 or higher\n",
        "\n",
        "Arrow está disponível como uma otimização ao converter um Spark DataFrame para um Pandas DataFrame usando a call ``DataFrame.toPandas()`` ou o inverso ``SparkSession.createDataFrame()``.\n",
        "\n",
        "Precisamos configurar o Spark Arrow para otimizar as calls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7MVV1MFsrFh"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "\n",
        "# SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HY3LUMNt_ta"
      },
      "source": [
        "# configurar o Arrow\n",
        "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enable\", True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ox26LBSqsmGs",
        "outputId": "6beebbd9-a1ab-4306-df84-b5d7eb9674d0"
      },
      "source": [
        "df = spark.createDataFrame([\n",
        "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
        "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
        "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
        "\n",
        "df.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+------+---+---+\n",
            "|color| fruit| v1| v2|\n",
            "+-----+------+---+---+\n",
            "|  red|banana|  1| 10|\n",
            "| blue|banana|  2| 20|\n",
            "|  red|carrot|  3| 30|\n",
            "| blue| grape|  4| 40|\n",
            "|  red|carrot|  5| 50|\n",
            "|black|carrot|  6| 60|\n",
            "|  red|banana|  7| 70|\n",
            "|  red| grape|  8| 80|\n",
            "+-----+------+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMVlQqI2wncY"
      },
      "source": [
        "A transformação é a mesma mesmo quando o Arrow está desativado. Porém é otimizada quando ativado.\n",
        "\n",
        "Nem todos os tipos de dados são suportados. Isso pode levar a um erro durante a conversão. Então o Spark irá retornar o processo e converter sem o Arrow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92ZsjwIgswu8"
      },
      "source": [
        "# Pandas DF\n",
        "pandas_df = pd.DataFrame(np.random.rand(100,3))\n",
        "\n",
        "# Spark DF\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "\n",
        "# converter Spark DF para Pandas DF usando Arrow\n",
        "arrow_pdf = df.select(\"*\").toPandas()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "d47uwIOswDuf",
        "outputId": "fba38091-8d65-4788-9b4b-195a52ec8c1f"
      },
      "source": [
        "arrow_pdf"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.708371</td>\n",
              "      <td>0.538061</td>\n",
              "      <td>0.079359</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.311344</td>\n",
              "      <td>0.981666</td>\n",
              "      <td>0.180534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.134101</td>\n",
              "      <td>0.830222</td>\n",
              "      <td>0.736917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.698400</td>\n",
              "      <td>0.580080</td>\n",
              "      <td>0.775096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.674968</td>\n",
              "      <td>0.250627</td>\n",
              "      <td>0.403149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0.611099</td>\n",
              "      <td>0.823814</td>\n",
              "      <td>0.407274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0.631503</td>\n",
              "      <td>0.767443</td>\n",
              "      <td>0.700638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.371547</td>\n",
              "      <td>0.252529</td>\n",
              "      <td>0.583755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>0.141851</td>\n",
              "      <td>0.683160</td>\n",
              "      <td>0.443891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>0.234336</td>\n",
              "      <td>0.036191</td>\n",
              "      <td>0.982162</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           0         1         2\n",
              "0   0.708371  0.538061  0.079359\n",
              "1   0.311344  0.981666  0.180534\n",
              "2   0.134101  0.830222  0.736917\n",
              "3   0.698400  0.580080  0.775096\n",
              "4   0.674968  0.250627  0.403149\n",
              "..       ...       ...       ...\n",
              "95  0.611099  0.823814  0.407274\n",
              "96  0.631503  0.767443  0.700638\n",
              "97  0.371547  0.252529  0.583755\n",
              "98  0.141851  0.683160  0.443891\n",
              "99  0.234336  0.036191  0.982162\n",
              "\n",
              "[100 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzBMiKnixVAC"
      },
      "source": [
        "## Pandas UDF (Vectorized User-Defined-Function)\n",
        "\n",
        "Funções definidas pelo usuário que são executados no spark utilizando o Arrow. Assim podemos trabalhar com pandas, o que permite realizar operações vetorizadas.\n",
        "\n",
        "Usa o ``pandas_udf()`` para embrulhar a função."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHMQPQMuwQHR",
        "outputId": "d4130c79-50d5-4deb-a5da-acf127a3efc0"
      },
      "source": [
        "@pandas_udf(\"col1 string, col2 long\")\n",
        "def func(s1: pd.Series, s2: pd.Series, s3: pd.DataFrame) -> pd.DataFrame:\n",
        "    s3['col2'] = s1 + s2.str.len()\n",
        "    return s3\n",
        "\n",
        "# Create a Spark DataFrame that has three columns including a struct column.\n",
        "df = spark.createDataFrame(\n",
        "    [[1, \"a string\", (\"a nested string\",)]],\n",
        "    \"long_col long, string_col string, struct_col struct<col1:string>\")\n",
        "\n",
        "df.printSchema()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- long_col: long (nullable = true)\n",
            " |-- string_col: string (nullable = true)\n",
            " |-- struct_col: struct (nullable = true)\n",
            " |    |-- col1: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lRlW2vxyghe",
        "outputId": "3526c3cc-f7a2-49a2-df91-017ae0ec666a"
      },
      "source": [
        "df.select(func(\"long_col\", \"string_col\", \"struct_col\")).printSchema()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- func(long_col, string_col, struct_col): struct (nullable = true)\n",
            " |    |-- col1: string (nullable = true)\n",
            " |    |-- col2: long (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9YaQcSyyk0B",
        "outputId": "13ffd1da-6b51-4ca9-9cdf-6242579d3f92"
      },
      "source": [
        "df.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+----------+-----------------+\n",
            "|long_col|string_col|       struct_col|\n",
            "+--------+----------+-----------------+\n",
            "|       1|  a string|{a nested string}|\n",
            "+--------+----------+-----------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eqgDANNy1pl",
        "outputId": "7ad6d8e1-a480-4430-db29-82babb26888d"
      },
      "source": [
        "from pyspark.sql.types import LongType\n",
        "\n",
        "# Declara a função\n",
        "def multiply_func(a: pd.Series, b: pd.Series) -> pd.Series:\n",
        "    return a * b\n",
        "\n",
        "# Cria UDF\n",
        "multiply = pandas_udf(multiply_func, returnType=LongType())\n",
        "\n",
        "# Pandas Series\n",
        "x = pd.Series([1, 2, 3])\n",
        "\n",
        "# Aplica a Função\n",
        "print(\"Função no Pandas DF\")\n",
        "print(multiply_func(x, x))\n",
        "\n",
        "# Cria Spark DF\n",
        "df = spark.createDataFrame(pd.DataFrame(x, columns=[\"x\"]))\n",
        "\n",
        "# Aplica a função no Spark DF com o UDF\n",
        "print(\"\\nFunção no Spark DF\")\n",
        "df.select(multiply(df[\"x\"], df[\"x\"])).show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Função no Pandas DF\n",
            "0    1\n",
            "1    4\n",
            "2    9\n",
            "dtype: int64\n",
            "\n",
            "Função no Spark DF\n",
            "+-------------------+\n",
            "|multiply_func(x, x)|\n",
            "+-------------------+\n",
            "|                  1|\n",
            "|                  4|\n",
            "|                  9|\n",
            "+-------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTlTzPUh0fRW"
      },
      "source": [
        "### Iterador -> Iterador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFvy3n0R0eth",
        "outputId": "2b0eb952-1cd0-4beb-fdea-77cbc0a0f311"
      },
      "source": [
        "from typing import Iterator\n",
        "\n",
        "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
        "df = spark.createDataFrame(pdf)\n",
        "\n",
        "# Função\n",
        "@pandas_udf(\"long\")\n",
        "def plus_one(iterator: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
        "    for x in iterator:\n",
        "        yield x + 1\n",
        "\n",
        "# Aplica Função\n",
        "df.select(plus_one(\"x\")).show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------+\n",
            "|plus_one(x)|\n",
            "+-----------+\n",
            "|          2|\n",
            "|          3|\n",
            "|          4|\n",
            "+-----------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd6NqZHw2XFG"
      },
      "source": [
        "### Iterator Multipla Série -> Iterator Múltipla Série"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuMlEGhz1h9S",
        "outputId": "3883c9ac-afcb-4a26-e5b2-5cf2974baae7"
      },
      "source": [
        "from typing import Iterator, Tuple\n",
        "\n",
        "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
        "df = spark.createDataFrame(pdf)\n",
        "\n",
        "# Função\n",
        "@pandas_udf(\"long\")\n",
        "def multiply_two_cols(\n",
        "        iterator: Iterator[Tuple[pd.Series, pd.Series]]) -> Iterator[pd.Series]:\n",
        "    for a, b in iterator:\n",
        "        yield a * b\n",
        "\n",
        "# Aplica função\n",
        "df.select(multiply_two_cols(\"x\", \"x\")).show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------+\n",
            "|multiply_two_cols(x, x)|\n",
            "+-----------------------+\n",
            "|                      1|\n",
            "|                      4|\n",
            "|                      9|\n",
            "+-----------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-Og6Lg93ENc"
      },
      "source": [
        "### Series -> Scalar"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPf_kyjE2iAt",
        "outputId": "de7754f0-4070-4134-b4e9-e27f7d1025ca"
      },
      "source": [
        "from pyspark.sql import Window\n",
        "\n",
        "df = spark.createDataFrame(\n",
        "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
        "    (\"id\", \"v\"))\n",
        "\n",
        "# Função\n",
        "@pandas_udf(\"double\")\n",
        "def mean_udf(v: pd.Series) -> float:\n",
        "    return v.mean()\n",
        "\n",
        "# Spark DF\n",
        "df.show()\n",
        "\n",
        "# Aplica Função\n",
        "df.select(mean_udf(df['v'])).show()\n",
        "\n",
        "# Aplica função com agregação\n",
        "df.groupby(\"id\").agg(mean_udf(df['v'])).show()\n",
        "\n",
        "\n",
        "w = Window \\\n",
        "    .partitionBy('id') \\\n",
        "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
        "\n",
        "df.withColumn('mean_v', mean_udf(df['v']).over(w)).show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+\n",
            "| id|   v|\n",
            "+---+----+\n",
            "|  1| 1.0|\n",
            "|  1| 2.0|\n",
            "|  2| 3.0|\n",
            "|  2| 5.0|\n",
            "|  2|10.0|\n",
            "+---+----+\n",
            "\n",
            "+-----------+\n",
            "|mean_udf(v)|\n",
            "+-----------+\n",
            "|        4.2|\n",
            "+-----------+\n",
            "\n",
            "+---+-----------+\n",
            "| id|mean_udf(v)|\n",
            "+---+-----------+\n",
            "|  1|        1.5|\n",
            "|  2|        6.0|\n",
            "+---+-----------+\n",
            "\n",
            "+---+----+------+\n",
            "| id|   v|mean_v|\n",
            "+---+----+------+\n",
            "|  1| 1.0|   1.5|\n",
            "|  1| 2.0|   1.5|\n",
            "|  2| 3.0|   6.0|\n",
            "|  2| 5.0|   6.0|\n",
            "|  2|10.0|   6.0|\n",
            "+---+----+------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DzLDYYT5d_3"
      },
      "source": [
        "## Funções Pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nmEq66-5hnJ"
      },
      "source": [
        "### Grouped Map\n",
        "\n",
        "``df.groupby().applyInPandas()``: implementa \"split-apply-combine\".\n",
        "\n",
        "Requer uma função python e o tipo de dado para o schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaPL_7cl3hZI",
        "outputId": "37e7ed0f-61ef-4065-b82c-14810e4775b1"
      },
      "source": [
        "df = spark.createDataFrame(\n",
        "    [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],\n",
        "    (\"id\", \"v\"))\n",
        "\n",
        "def subtract_mean(pdf):\n",
        "    # pdf é um Pandas DF\n",
        "    v = pdf.v\n",
        "    return pdf.assign(v=v - v.mean())\n",
        "\n",
        "df.show()\n",
        "\n",
        "df.groupby(\"id\").applyInPandas(subtract_mean, schema=\"id long, v double\").show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+----+\n",
            "| id|   v|\n",
            "+---+----+\n",
            "|  1| 1.0|\n",
            "|  1| 2.0|\n",
            "|  2| 3.0|\n",
            "|  2| 5.0|\n",
            "|  2|10.0|\n",
            "+---+----+\n",
            "\n",
            "+---+----+\n",
            "| id|   v|\n",
            "+---+----+\n",
            "|  1|-0.5|\n",
            "|  1| 0.5|\n",
            "|  2|-3.0|\n",
            "|  2|-1.0|\n",
            "|  2| 4.0|\n",
            "+---+----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjc4Uh5r6n8g"
      },
      "source": [
        "### Map\n",
        "\n",
        "``df.mapInPandas()``: mapeia um iterador do pandas df para outro iterador do pandas df e retorna um spark df."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3rxbGpm5-EO",
        "outputId": "3e671e06-bd89-4ecc-c40c-d7f7bb99f578"
      },
      "source": [
        "df = spark.createDataFrame([(1, 21), (2, 30)], (\"id\", \"age\"))\n",
        "\n",
        "def filter_func(iterator):\n",
        "    for pdf in iterator:\n",
        "        yield pdf[pdf.id == 1]\n",
        "\n",
        "df.mapInPandas(filter_func, schema=df.schema).show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "| id|age|\n",
            "+---+---+\n",
            "|  1| 21|\n",
            "+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCwHHWzh7Nei"
      },
      "source": [
        "### Co-Grouped Map\n",
        "\n",
        "``df.groupby().cogroup().applyInPandas()``: permite o agrupamento entre dois sparks df através de uma chave comum. A função Python é aplicada a cada grupo.\n",
        "\n",
        "Requer uma função que define a computação pra cada grupo e os tipos de dados do schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHKvc7DF653H",
        "outputId": "833a3596-15f5-4dae-cb3d-e14db8ba30af"
      },
      "source": [
        "df1 = spark.createDataFrame(\n",
        "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
        "    (\"time\", \"id\", \"v1\"))\n",
        "\n",
        "df2 = spark.createDataFrame(\n",
        "    [(20000101, 1, \"x\"), (20000101, 2, \"y\")],\n",
        "    (\"time\", \"id\", \"v2\"))\n",
        "\n",
        "def asof_join(l, r):\n",
        "    return pd.merge_asof(l, r, on=\"time\", by=\"id\")\n",
        "\n",
        "df1.show()\n",
        "\n",
        "df2.show()\n",
        "\n",
        "df1.groupby(\"id\").cogroup(df2.groupby(\"id\")).applyInPandas(\n",
        "    asof_join, schema=\"time int, id int, v1 double, v2 string\").show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------+---+---+\n",
            "|    time| id| v1|\n",
            "+--------+---+---+\n",
            "|20000101|  1|1.0|\n",
            "|20000101|  2|2.0|\n",
            "|20000102|  1|3.0|\n",
            "|20000102|  2|4.0|\n",
            "+--------+---+---+\n",
            "\n",
            "+--------+---+---+\n",
            "|    time| id| v2|\n",
            "+--------+---+---+\n",
            "|20000101|  1|  x|\n",
            "|20000101|  2|  y|\n",
            "+--------+---+---+\n",
            "\n",
            "+--------+---+---+---+\n",
            "|    time| id| v1| v2|\n",
            "+--------+---+---+---+\n",
            "|20000101|  1|1.0|  x|\n",
            "|20000102|  1|3.0|  x|\n",
            "|20000101|  2|2.0|  y|\n",
            "|20000102|  2|4.0|  y|\n",
            "+--------+---+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaOOZ2fP7rFw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}